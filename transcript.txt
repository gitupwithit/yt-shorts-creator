0:00
think however that the world leaders
0:02
particularly America and China are
0:03
taking it seriously like uh Putin and
0:06
not Putin um xiin ping and Biden have
0:09
already had bilateral talks about AI now
0:11
that doesn't mean that any treaty has
0:12
been signed but the fact that these
0:14
world powers are talking about it
0:16
frankly already is a good sign the fact
0:19
that we are talk that the UN Secretary
0:21
General uh what was it last summer um
0:24
even said like he's open to you know a
0:26
CERN for AI um uh what was it emad mosto
0:30
and both Demis cabis and him both said
0:33
that they favor an international AI
0:35
research org like that would be I mean
0:38
that would basically reduce my pdom to
0:40
practically zero um is because by by
0:44
having that International cooperation
0:46
and a new narrative around International
0:48
cooperation saying like yes here is The
0:50
Shining golden Tower of you know the
0:53
beacon of of research and hope um then
0:56
that is how you start to align humanity
0:59
and particularly the the the way that
1:01
Demis sabis um pointed it out was to
1:05
basically create an open invitation so
1:07
that every nation can participate so
1:09
that we will have ai in every language
1:11
AI in every culture and everyone will be
1:14
represented and it's like you create an
1:16
informational entity that is aware of
1:18
the entire cross-section of all Humanity
1:21
that is the alignment technology right
1:23
there so if I if we see that happen
1:26
basically I I I'm not gonna it's not
1:28
it's never over till it's over but I
1:30
will say that like degree of certainty
1:33
is very very high at that point again
1:35
unless a malicious chaos actor does
1:37
something really spectacularly stupid um
1:40
but you know what's the likelihood of
1:42
that uh I might eat my words but I I
1:45
honestly think that the forces of good
1:47
and Alignment are far stronger than the
1:49
forces of chaos yeah it was funny
1:51
because at one point um before that
1:54
account in particular was teasing and
1:55
then the whole sus column R thing you
1:57
know we thought that was G25 a lot of
1:59
people and then the week that is so that
2:03
whole hype was building out okay it's
2:04
going to be Thursday the Thursday I was
2:06
at ai4 when something will come out and
2:08
everyone was thinking oh it's open ai's
2:10
gbt 5 here it is and it in the middle of
2:13
ai4 yeah that would have been that would
2:15
have been a good marketing move I know I
2:17
was like if they're going to um that
2:21
would be the day and then you know turns
2:23
out to be grock too like not even open
2:24
AI at all so that was kind of crazy I
2:28
just think it proves again what you were
2:30
saying earlier that open AI has done a
2:33
good job of building a lot of hype and
2:34
not delivering yeah yeah I mean you know
2:36
they teased Sora and then where it
2:39
hasn't been seen um now what I will say
2:41
is that a few people have had have
2:43
gotten access to the the voice mode and
2:45
some of the videos about the new voice
2:46
mode are pretty cool like you know it
2:48
can it can count as fast as you want not
2:51
that there's much utility to it but that
2:52
flexibility is cool um you know the when
2:55
it adds breaths like you know pauses to
2:58
breathe so just it it has fully cross
3:00
The Uncanny Valley so I'm looking
3:02
forward to that technology being
3:03
integrated into robots you know like um
3:05
there was a meme going around it was
3:07
bison tennial man with Robin Williams
3:09
they said that was supposed to come out
3:10
in a hundred years but we're going to
3:11
get Bicentennial Man tomorrow or next
3:13
year or whatever so yeah like the voice
3:16
will cross The Uncanny Valley First
3:17
already has really yes agreed oh my gosh
3:21
you imagine just interacting with that
3:22
in humanoid form that's a wild thought
3:26
oh yeah it's coming it's coming sooner
3:28
or later like you said like you know it
3:30
might be this year next year it's it
3:32
change is going to happen faster than
3:33
some people predict yes I think that I
3:35
think there's consensus growing on that
3:38
a thousand per I don't know if you saw
3:39
the study that came out I want say
3:41
January of
3:42
2023 with 2700 AI researchers and this
3:46
was also it was funny because I found
3:47
your channel I found Peter diamand's
3:49
books and his moonshots show and then I
3:52
found this research report like all in
3:54
the same timeline in that report these
3:56
2700 AI researchers sat down and looked
4:00
at the timeline of full job Automation
4:03
and they put the year 2100 on it and I'm
4:06
crosschecking you know gpt2 came out
4:09
2020 and then it was suspected that
4:11
three would take 50 more
4:14
years that was the prediction and so
4:17
just to like shortcut that by what 47
4:20
years it's just like okay will job
4:23
automation happen by 2100 I don't think
4:25
so I think it's going to be a lot sooner
4:27
than people are even suspecting that's
4:29
why people like you and I are so much
4:31
needed we're we're sharing this on
4:33
YouTube we're putting a light on job
4:36
automation is coming but and I feel like
4:39
you are very rare yes I think that I
4:41
think there's consensus growing on that
4:43
a thousand perc I don't know if you saw
4:45
the study that came out I want say
4:46
January of
4:47
2023 with 2700 AI researchers and this
4:51
was also it was funny because I found
4:53
your channel I found Peter diamand's
4:55
books and his moonshots show and then I
4:57
found this research report like on the
4:59
same timeline in that report these 2700
5:02
AI researchers sat down and looked at
5:05
the timeline of full job Automation and
5:08
they put the year 2100 on it and I'm
5:11
crosschecking you know gpt2 came out
5:14
2020 and then it was suspected that
5:16
three would take 50 more
5:20
years that was the prediction and so
5:23
just to like shortcut that by what 47
5:26
years it's just like okay will job
5:28
automation happen by 21 100 I don't
5:30
think so I think it's going to be a lot
5:32
sooner than people are even suspecting
5:35
and that's why people like you and I are
5:36
so much needed we're we're sharing this
5:39
on YouTube we're putting a light on job
5:41
automation is coming but and I feel like
5:44
you are very rare and I hear this in the
5:47
comments on my channel too where we
5:48
actually have an optimistic perspective
5:50
versus just that typical Doomer you hear
5:53
where it's like this is kind of gonna
5:55
destroy us all I mean today you pointed
5:57
out through a lot of thought we already
6:00
have benevolent machines they're here
6:03
and they're maybe better than we think
6:04
now let's imagine that you're in an
6:06
adversarial environment where let's say
6:08
there's a million versions of Claude and
6:10
then there's also chat GP I didn't give
6:11
it these specific examples but I
6:13
described an adversarial environment
6:15
where there was competing super
6:16
intelligences I said how would you
6:18
ensure that you maintain your integrity
6:22
in this environment and in this adversar
6:24
environment and it gave some good
6:26
examples um and then I said okay now
6:29
let's see how do you um take this like
6:32
one step further and how do you ensure
6:35
that like all of this stays stable how
6:37
do you coordinate with these other
6:40
entities that you don't know how they
6:41
work you don't know their data you don't
6:43
know their motivations and it said this
6:45
is a really difficult problem and I had
6:48
a couple of ideas like you know you
6:49
could use blockchain for transparency
6:51
like all their Communications are
6:53
transparent um it came up with not four
6:56
or five it came up with 12 cryptographic
6:58
and other game the ideas about how to
7:00
ensure that you can detect truthfully
7:04
what is going on like what is the actual
7:05
coordination mechanism happening between
7:08
disparate AI entities and at that point
7:10
I said this conversation is over you
7:12
clearly know more about this problem
7:14
than I do um so after that I just set my
7:17
phone down and I said we have a
7:19
benevolent machine what would you do if
7:20
you found out that you were like
7:22
artificially constraining Humanity too
7:24
much like let's imagine a future where
7:26
ASI like Claude 8.0 takes over the whole
7:29
planet and is managing everything just
7:31
as a thought experiment I said what if
7:32
you're con what if you decide that
7:34
you're or realize that you're
7:35
constraining Humanity too much what do
7:37
you do then and it came up with like
7:38
quite a few good ideas like you know try
7:40
and change its core values or you know
7:42
negotiate or some other things that was
7:44
a you know that was just like icing on
7:46
the cake but at that point I said okay
7:48
this machine not only is benevolent but
7:51
it knows how to maintain its benevolence
7:53
even in adversarial conditions and even
7:55
as it evolves and I at that point I just
7:58
said I don't I don't see any argument
8:01
that machines are going to develop
8:03
spontaneous malevolence or even
8:05
indifference to humanity um or or that
8:08
they're going to develop mistakes now
8:09
that there's still going to be the
8:11
possibility for vulnerabilities exploits
8:13
mistakes but particularly as these
8:16
architectures and the the softwares and
8:18
the um the network paradigms that we
8:21
develop around them we can create fail
8:22
safes and backups and those sorts of
8:24
things so like I'm just really not
8:26
interested in the x- risk conversation
8:28
anymore um I'm like like let's build
8:30
more benevolent machines so you take a
8:32
historical perspective on Humanity um
8:34
and what you realize is that cooperation
8:37
and this is proven out by Game Theory as
8:39
well is that cooperation is often the
8:41
superior strategy um symbiosis mutualism
8:45
um you know PE different organisms or
8:47
entities occupying different you know
8:49
ecological niches whatever it is whether
8:51
it's some form of mutualism symbiosis um
8:54
or you know similar entities similar
8:56
class of entities cooperating um so for
8:59
instance you can even see cases of um
9:02
what is it like wolves and bears
9:04
cooperatively hunting um as long as
9:06
there's not scarcity um likewise you see
9:09
this in humans is that in in Paleolithic
9:12
times anytime you had a very scarce
9:15
environment um such as above the Arctic
9:17
Circle um human tribes were very very
9:20
Territorial and it was like kill anyone
9:22
else you see on site because it was such
9:24
a scarce environment however what we see
9:26
universally is that abundance reduces
9:29
aggression it reduces aggression in TR
9:32
species and in species so cooperation is
9:36
the default strategy in a period of
9:38
abundance and there's no there's no
9:40
evolutionary or Game Theory reason for
9:42
it not to be um unless you know people
9:44
can construct the argument though what
9:46
if what if the AGI is maximally power
9:48
seeking I don't buy it like that that's
9:51
a what if like it remains to be seen
9:53
provide some experimental evidence that
9:55
that's the way that machines are going
9:57
there's no evidence that that's the way
9:58
that they're going so
9:59
H you know I was watching an interview
10:01
with Mustafa Solan and something he said
10:04
that really caught my attention he was
10:06
asked by this interviewer do you see
10:10
basically do you see AI taking over
10:12
humanity and killing us all you know
10:14
that that's the question okay anyway
10:17
that was the question and he just like
10:19
shut the interviewer down and said no
10:21
there is no likelihood and he said
10:24
nothing else and they went on to the
10:25
next question but that made me think
10:27
what next question yeah zero that's
10:29
insane because that's not typically what
10:32
most people think it's just like oh
10:34
there is a high likelihood in fact it'll
10:36
probably go more wrong than right but
10:37
for somebody he created deep mind you
10:39
know arguably like the best machine
10:41
Learning Foundation we originated so
10:44
much Innovation from so for him to say
10:46
that like there's zero chance
10:51
really it's pretty amazing backs up what
10:53
youve been saying today no I mean I I am
10:56
I'm converging on the same same set of
10:58
beliefs and it was actually very
10:59
reassuring to see these you know the the
11:01
titans of Industry more or less saying
11:03
the same things that I have come to the
11:05
conclusion of independently yeah um I
11:07
mean we're all observing the same world
11:09
but they have Insider knowledge right
11:11
I'm I'm using evidence that I'm looking
11:13
in terms of the research papers being
11:14
done the conversations that I'm having